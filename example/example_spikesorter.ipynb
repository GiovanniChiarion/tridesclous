{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary for having figures directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "from tridesclous import SpikeSorter, SpikeSortingWindow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download Locust dataset from zenedo.\n",
    "We take 3 trials in this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "name = 'locust20010201.hdf5'\n",
    "distantfile = 'https://zenodo.org/record/21589/files/'+name\n",
    "localfile = name\n",
    "if not os.path.exists(localfile):\n",
    "    urlretrieve(distantfile, localfile)\n",
    "hdf = h5py.File(localfile,'r')\n",
    "\n",
    "# read 3 trials (=3 segments)\n",
    "ch_names = ['ch09','ch11','ch13','ch16']\n",
    "trial_names = ['trial_01', 'trial_02', 'trial_03']\n",
    "sigs_by_trials = []\n",
    "for trial_name in trial_names:\n",
    "    sigs = np.array([hdf['Continuous_1'][trial_name][name][...] for name in ch_names]).transpose()\n",
    "    sigs = (sigs.astype('float32') - 2**15.) / 2**15\n",
    "    sigs_by_trials.append(sigs)\n",
    "\n",
    "sampling_rate = 15000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpikeSorter: insert several segments\n",
    "\n",
    "Create our data manager and append some data into it.\n",
    "Note that our data is already filtered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "data already in store for seg_num 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fc170f5975b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msigs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigs_by_trials\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseg_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     spikesorter.dataio.append_signals(sigs, seg_num = seg_num,t_start = 0.+5*seg_num, \n\u001b[1;32m----> 6\u001b[1;33m                 sampling_rate =  sampling_rate, already_hp_filtered = True, channels = ch_names)\n\u001b[0m",
      "\u001b[1;32m/home/sgarcia/Documents/projet/tridesclous/tridesclous/dataio.py\u001b[0m in \u001b[0;36mappend_signals\u001b[1;34m(self, signals, seg_num, sampling_rate, t_start, already_hp_filtered, channels)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m#this check overlap if trying to write an already exisiting chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# theorically this should work but the index will unefficient when self.store.select\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseg_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m't_start'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseg_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m't_stop'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'data already in store for seg_num {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: data already in store for seg_num 0"
     ]
    }
   ],
   "source": [
    "spikesorter = SpikeSorter(dirname = 'Dataset several segment')\n",
    "\n",
    "for seg_num in range(3):\n",
    "    sigs = sigs_by_trials[seg_num]\n",
    "    spikesorter.dataio.append_signals(sigs, seg_num = seg_num,t_start = 0.+5*seg_num, \n",
    "                sampling_rate =  sampling_rate, already_hp_filtered = True, channels = ch_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIO <139896538925712>\n",
      "Workdir: Dataset several segment/data.h5\n",
      "sampling_rate: 15000.0\n",
      "nb_channel: 4\n",
      "nb_segments: 3\n",
      "Segment 0\n",
      "    duration : 28.7698s.\n",
      "    times range : 0.0 - 28.7698\n",
      "    nb_peaks : 1078\n",
      "Segment 1\n",
      "    duration : 28.769800000000004s.\n",
      "    times range : 5.0 - 33.769800000000004\n",
      "    nb_peaks : 1140\n",
      "Segment 2\n",
      "    duration : 28.769800000000004s.\n",
      "    times range : 10.0 - 38.769800000000004\n",
      "    nb_peaks : 899\n",
      "Peak Cluster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spikesorter.summary(level=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Detect peak an dextract waveform in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIO <139896538925712>\n",
      "Workdir: Dataset several segment/data.h5\n",
      "sampling_rate: 15000.0\n",
      "nb_channel: 4\n",
      "nb_segments: 3\n",
      "Segment 0\n",
      "    duration : 28.7698s.\n",
      "    times range : 0.0 - 28.7698\n",
      "    nb_peaks : 1078\n",
      "Segment 1\n",
      "    duration : 28.769800000000004s.\n",
      "    times range : 5.0 - 33.769800000000004\n",
      "    nb_peaks : 1140\n",
      "Segment 2\n",
      "    duration : 28.769800000000004s.\n",
      "    times range : 10.0 - 38.769800000000004\n",
      "    nb_peaks : 899\n",
      "Peak Cluster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spikesorter.detect_peaks_extract_waveforms(seg_nums = 'all',  threshold=-4, \n",
    "                                    peak_sign = '-', n_span = 2, n_left=-30, n_right=50)\n",
    "print(spikesorter.summary(level=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project and find cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIO <139896538925712>\n",
      "Workdir: Dataset several segment/data.h5\n",
      "sampling_rate: 15000.0\n",
      "nb_channel: 4\n",
      "nb_segments: 3\n",
      "Segment 0\n",
      "    duration : 28.7698s.\n",
      "    times range : 0.0 - 28.7698\n",
      "    nb_peaks : 1078\n",
      "Segment 1\n",
      "    duration : 28.769800000000004s.\n",
      "    times range : 5.0 - 33.769800000000004\n",
      "    nb_peaks : 1140\n",
      "Segment 2\n",
      "    duration : 28.769800000000004s.\n",
      "    times range : 10.0 - 38.769800000000004\n",
      "    nb_peaks : 899\n",
      "Peak Cluster\n",
      "  #0: 255\n",
      "  #1: 1303\n",
      "  #2: 384\n",
      "  #3: 116\n",
      "  #4: 210\n",
      "  #5: 206\n",
      "  #6: 642\n",
      "\n",
      "label\n",
      "0     255\n",
      "1    1303\n",
      "2     384\n",
      "3     116\n",
      "4     210\n",
      "5     206\n",
      "6     642\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "spikesorter.project(method = 'pca', n_components = 5)\n",
    "spikesorter.find_clusters(7)\n",
    "spikesorter.refresh_colors(reset=True, palette = 'husl')\n",
    "print(spikesorter.summary(level=1))\n",
    "print(spikesorter.cluster_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open interactive windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%gui qt4\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = SpikeSortingWindow(spikesorter)\n",
    "win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *tridesclous* example with locust dataset\n",
    "\n",
    "Here a detail notebook that detail the locust dataset recodring by Christophe Pouzat.\n",
    "\n",
    "This dataset is our classic.\n",
    "It has be analyse yet by several tools in R, Python or C:\n",
    "  * https://github.com/christophe-pouzat/PouzatDetorakisEuroScipy2014\n",
    "  * https://github.com/christophe-pouzat/SortingABigDataSetWithPython\n",
    "  * http://xtof.perso.math.cnrs.fr/locust.html\n",
    "\n",
    "So we can compare the result.\n",
    "\n",
    "The original datasets is here https://zenodo.org/record/21589\n",
    "\n",
    "But we will work on a very small subset on github https://github.com/tridesclous/tridesclous_datasets/tree/master/locust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In *tridesclous*, the spike sorting is done in several step:\n",
    "  * Define the datasource and working path. (class DataIO)\n",
    "  * Construct a *catalogue* (class CatalogueConstructor) on a short chunk of data (for instance 60s)\n",
    "    with several sub step :\n",
    "    * signal pre-processing:\n",
    "      * high pass filter (optional)\n",
    "      * removal of common reference (optional)\n",
    "      * noise estimation (median/mad) on a small chunk\n",
    "      * normalisation = robust z-score\n",
    "    * peak detection\n",
    "    * extract some waveform. Unecessary and impossible to extract them all.\n",
    "    * find rational limit of waveforms (n_left/n_right)\n",
    "    * project theses waveforms in smaller dimention (pca, ...)\n",
    "    * find cluster\n",
    "    * clean with GUI (class CatalogueWindow)\n",
    "    * save centroids (median+mad + first and second derivative)\n",
    "  * Apply the *Peeler* (class Peeler) on the long term signals. With several sub steps:\n",
    "     * same signal preprocessing than before\n",
    "     * find peaks\n",
    "     * find the best cluster in catalogue for each peak\n",
    "     * find the intersample jitter\n",
    "     * remove the oversampled waveforms from the signals until there are not peaks in the signals.\n",
    "     * check with GUI (class PeelerWindow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tridesclous as tdc\n",
    "\n",
    "from tridesclous import DataIO, CatalogueConstructor, Peeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download a small dataset\n",
    "\n",
    "trideclous provide some datasets than can be downloaded with **download_dataset**.\n",
    "\n",
    "Note this dataset contains 2 trials in 2 different files. (the original contains more!)\n",
    "\n",
    "Each file is considers as a *segment*. *tridesclous* automatically deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locust_trial_01.raw\n",
      "locust_trial_02.raw\n",
      "['/home/samuel/Documents/projet/tridesclous/example/locust/locust_trial_01.raw', '/home/samuel/Documents/projet/tridesclous/example/locust/locust_trial_02.raw']\n",
      "{'sample_rate': 15000.0, 'total_channel': 4, 'dtype': 'int16'}\n"
     ]
    }
   ],
   "source": [
    "#download dataset\n",
    "localdir, filenames, params = tdc.download_dataset(name='locust')\n",
    "print(filenames)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataIO = define datasource and working dir\n",
    "\n",
    "\n",
    "Theses 2 files are in **RawData** format this means binary format with interleaved channels.\n",
    "\n",
    "Our dataset contains 2 segment of 28.8 second each, 4 channels. The sample rate is 15kHz.\n",
    "\n",
    "Note that there is only one channel_group here (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataIO <id: 140673492078264> \n",
      "  workdir: tridesclous_locust\n",
      "  sample_rate: 15000.0\n",
      "  total_channel: 4\n",
      "  channel_groups: 0 [ch0 ch1 ch2 ch3]\n",
      "  nb_segment: 2\n",
      "  length: 431548 431548\n",
      "  durations: 28.8 28.8 s.\n"
     ]
    }
   ],
   "source": [
    "#create a DataIO\n",
    "import os, shutil\n",
    "dirname = 'tridesclous_locust'\n",
    "if os.path.exists(dirname):\n",
    "    #remove is already exists\n",
    "    shutil.rmtree(dirname)    \n",
    "dataio = DataIO(dirname=dirname)\n",
    "\n",
    "# feed DataIO\n",
    "dataio.set_data_source(type='RawData', filenames=filenames, **params)\n",
    "print(dataio)\n",
    "\n",
    "#no need to setup the prb with dataio.set_probe_file() or dataio.download_probe()\n",
    "#because it is a tetrode\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatalogueConstructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalogueConstructor <id: 140673492077760> \n",
      "  workdir: tridesclous_locust/channel_group_0/catalogue_constructor\n",
      "  Signal pre-processing not done yet\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor = CatalogueConstructor(dataio=dataio)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters for the pre-processing step.\n",
    "\n",
    "For a complet description of each params see main documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catalogueconstructor.set_preprocessor_params(chunksize=1024,\n",
    "            common_ref_removal=False,\n",
    "            highpass_freq=300.,\n",
    "            lowpass_freq=5000.,                                             \n",
    "            lostfront_chunksize=64,\n",
    "            peak_sign='-',\n",
    "            relative_threshold=6.5,\n",
    "            peak_span=0.0001,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the median and mad of noiseon a small chunk of filtered signals.\n",
    "This compute medians and mad of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.28773475  0.84435308  1.68706632  0.50887132]\n",
      "[ 51.0532341   46.69039154  57.44741058  44.83795547]\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor.estimate_signals_noise(seg_num=0, duration=15.)\n",
    "print(catalogueconstructor.signals_medians)\n",
    "print(catalogueconstructor.signals_mads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the main loop: signal preprocessing + peak detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_signalprocessor 4.866203265999502 s\n",
      "CatalogueConstructor <id: 140673492077760> \n",
      "  workdir: tridesclous_locust/channel_group_0/catalogue_constructor\n",
      "  nb_peak: 1323\n",
      "  nb_peak_by_segment: 646, 677\n",
      "  cluster_labels [-10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "catalogueconstructor.run_signalprocessor(duration=60.)\n",
    "t2 = time.perf_counter()\n",
    "\n",
    "print('run_signalprocessor', t2-t1, 's')\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract some waveforms\n",
    "\n",
    "Take some waveforms in the signals *n_left/n_right* must be choosen arbitrary but lon enought.\n",
    "Better limits will be set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalogueConstructor <id: 140673492077760> \n",
      "  workdir: tridesclous_locust/channel_group_0/catalogue_constructor\n",
      "  nb_peak: 1323\n",
      "  nb_peak_by_segment: 646, 677\n",
      "  n_left -25 n_right 40\n",
      "  some_waveforms.shape: (1323, 65, 4)\n",
      "  cluster_labels [-10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalogueconstructor.extract_some_waveforms(n_left=-25, n_right=40, mode='rand', nb_max=10000, align_waveform=True)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean waveforms\n",
    "\n",
    "Whis try to detect bad waveforms to not include them in features aand clustering.\n",
    "Strange waveforms are tag with -9 (alien)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catalogueconstructor.clean_waveforms(alien_value_threshold=100.)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find good limits for waveforms and re-extract\n",
    "\n",
    "To avoid useless portion of signal on the sides of peaks we take smaller sweep.\n",
    "This technics is based on the MAD. We take only central zone where the MAD is above the noise.\n",
    "Noise is 1. In practice we take a bit more 1.1\n",
    "\n",
    "Here the methods give a \"good limts\" of n_left -10 n_right 15.\n",
    "\n",
    "So the shape of waveforms become smaller.\n",
    "\n",
    "Note that this technic work well on tetrode or small channel number but for large array it is as good as manual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatalogueConstructor <id: 140673492077760> \n",
      "  workdir: tridesclous_locust/channel_group_0/catalogue_constructor\n",
      "  nb_peak: 1323\n",
      "  nb_peak_by_segment: 646, 677\n",
      "  n_left -11 n_right 13\n",
      "  some_waveforms.shape: (1323, 24, 4)\n",
      "  cluster_labels [-10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_left, n_right = catalogueconstructor.find_good_limits(mad_threshold = 1.1,)\n",
    "print(catalogueconstructor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project to smaller space\n",
    "\n",
    "To reduce dimension of the waveforms (1323, 24, 4) we chosse global_pac method which is appropriate for tetrode.\n",
    "It consists of flatenning some_waveforms.shape (1323, 24, 4) to (1323, 24x4) and then apply a standard PCA on it with sklearn.\n",
    "\n",
    "Let's keep 5 component of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project 0.05536318400118034\n",
      "CatalogueConstructor <id: 140673492077760> \n",
      "  workdir: tridesclous_locust/channel_group_0/catalogue_constructor\n",
      "  nb_peak: 1323\n",
      "  nb_peak_by_segment: 646, 677\n",
      "  n_left -11 n_right 13\n",
      "  some_waveforms.shape: (1323, 24, 4)\n",
      "  some_features.shape: (1323, 5)\n",
      "  cluster_labels [-10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "catalogueconstructor.extract_some_features(method='global_pca', n_components=5)\n",
    "t2 = time.perf_counter()\n",
    "print('project', t2-t1)\n",
    "print(catalogueconstructor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find clusters\n",
    "\n",
    "There are many option to cluster this features. here a simple one the well known kmeans method.\n",
    "\n",
    "Unfortunatly we need to choose the number of cluster. Too bad... Let's take 12.\n",
    "\n",
    "Later on we will be able to refine this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_centroid\n",
      "compute_centroid 0.016732463998778258\n",
      "waveforms_rms\n",
      "find_clusters 0.16835331099719042\n",
      "CatalogueConstructor <id: 140673492077760> \n",
      "  workdir: tridesclous_locust/channel_group_0/catalogue_constructor\n",
      "  nb_peak: 1323\n",
      "  nb_peak_by_segment: 646, 677\n",
      "  n_left -11 n_right 13\n",
      "  some_waveforms.shape: (1323, 24, 4)\n",
      "  some_features.shape: (1323, 5)\n",
      "  cluster_labels [ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "catalogueconstructor.find_clusters(method='kmeans', n_clusters=12)\n",
    "t2 = time.perf_counter()\n",
    "print('find_clusters', t2-t1)\n",
    "print(catalogueconstructor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open CatalogueWindow for visual check\n",
    "\n",
    "This open a CatalogueWindow, here we can check, split merge, trash, play as long as we are not happy.\n",
    "\n",
    "We happy, we can save the catalogue.\n",
    "\n",
    "Don't save nothing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute_centroid\n",
      "compute_centroid 0.01420532500196714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%gui qt5\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = tdc.CatalogueWindow(catalogueconstructor)\n",
    "win.show()\n",
    "app.exec_()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a snappshot of CatalogueWindow\n",
    "\n",
    "<img src=\"../doc/img/snapshot_cataloguewindow.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirty clean of catatalogue\n",
    "\n",
    "Here a quick and dirty clean of teh catalogue and them save it!!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveforms_rms\n",
      "construct_catalogue 0.06322626900509931\n"
     ]
    }
   ],
   "source": [
    "#order cluster by waveforms rms\n",
    "catalogueconstructor.order_clusters(by='waveforms_rms')\n",
    "\n",
    "#put label 0 to trash\n",
    "mask = catalogueconstructor.all_peaks['label'] == 0\n",
    "catalogueconstructor.all_peaks['label'][mask] = -1\n",
    "catalogueconstructor.on_new_cluster()\n",
    "\n",
    "#save the catalogue\n",
    "catalogueconstructor.make_catalogue_for_peeler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peeler\n",
    "\n",
    "Create and run the Peeler.\n",
    "It should be pretty fast, here the computation take 1.32s for 28.8x2s of signal. This is a speed up of 43 over real time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 421/421 [00:00<00:00, 541.19it/s]\n",
      "100%|██████████| 421/421 [00:00<00:00, 561.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peeler.run 1.68495810800232\n",
      "\n",
      "seg_num 0 nb_spikes 619\n",
      "seg_num 1 nb_spikes 656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "initial_catalogue = dataio.load_catalogue(chan_grp=0)\n",
    "\n",
    "peeler = Peeler(dataio)\n",
    "peeler.change_params(catalogue=initial_catalogue)\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "peeler.run()\n",
    "t2 = time.perf_counter()\n",
    "print('peeler.run', t2-t1)\n",
    "\n",
    "print()\n",
    "for seg_num in range(dataio.nb_segment):\n",
    "    spikes = dataio.get_spikes(seg_num)\n",
    "    print('seg_num', seg_num, 'nb_spikes', spikes.size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open PeelerWindow for visual checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%gui qt5\n",
    "import pyqtgraph as pg\n",
    "app = pg.mkQApp()\n",
    "win = tdc.PeelerWindow(dataio=dataio, catalogue=initial_catalogue)\n",
    "win.show()\n",
    "app.exec_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a snappshot of PeelerWindow\n",
    "\n",
    "<img src=\"../doc/img/snapshot_peelerwindow.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
